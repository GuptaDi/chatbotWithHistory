{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Chatbot\n",
    "We will design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions.\n",
    "\n",
    "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
    "\n",
    "- Conversational RAG: Enable a chatbot experience over an external source of data\n",
    "- Agents: Build a chatbot that can take actions\n",
    "\n",
    "This is a basic chatbot\n",
    "1. Load the environment variables from the .env file.\n",
    "2. Store these env variable in a separate variable used further to access the models.\n",
    "3. Select and load your model. In our case, we are using a Gemma model from chatgroq, langchain.\n",
    "4. Chatting with chatbot to see if it remembers previous history.\n",
    "\n",
    "##### In case of multiple users, we need to maintain session and session data for each chat.\n",
    "5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()   # loading all the environment variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Groq - AI inference for faster compute and memory\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x114ee8050>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x114ee8d70>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Divya Gupta, it's nice to meet you!  \\n\\nBeing a Lead Developer is a great role. What kind of projects do you typically work on?  \\n\\nI'm happy to chat about anything you'd like, whether it's about your work, technology, or just something else entirely. ðŸ˜Š\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 23, 'total_tokens': 92, 'completion_time': 0.125454545, 'prompt_time': 0.002121366, 'queue_time': 0.160019053, 'total_time': 0.127575911}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--da05660c-bfd7-4386-9c05-59d5b27d406b-0', usage_metadata={'input_tokens': 23, 'output_tokens': 69, 'total_tokens': 92})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with basic text\n",
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi , My name is Divya Gupta and I am a Lead Developer\")])\n",
    "\n",
    "# Here I have not passed anything from the SystemMessage. \n",
    "# SystemMessage will tell what kind of behaviour the LLM will perform.\n",
    "# AIMessage : is the response from LLM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Divya Gupta, and you are a Lead Developer!  \\n\\nIs there anything else you'd like to know or discuss? ðŸ˜Š  I'm here to help. \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 112, 'total_tokens': 153, 'completion_time': 0.074545455, 'prompt_time': 0.007408926, 'queue_time': 0.161532398, 'total_time': 0.081954381}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--a9ee539e-db64-40aa-9f18-a78c3c3df157-0', usage_metadata={'input_tokens': 112, 'output_tokens': 41, 'total_tokens': 153})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are giving a list of messages. And giving some previous information.\n",
    "# The chatbot is remembering the previous information too.\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi , My name is Divya Gupta and I am a Lead Developer\"),\n",
    "        AIMessage(content=\"Hi Divya Gupta, it's nice to meet you!  \\n\\nBeing a Lead Developer is a great role. What kind of projects do you typically work on?  \\n\\nI'm happy to chat about anything you'd like, whether it's about your work, technology, or just something else entirely. ðŸ˜Š\\n\"),\n",
    "        HumanMessage(content=\"Hey What's my name and what do I do?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History\n",
    "We can use a Message History class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory #every next message should be the part of the previous conversation.\n",
    "\n",
    "store={}\n",
    "\n",
    "# Distinguish one chat session with the other\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory() # whatever chat is happening will go to this session id.\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)\n",
    "\n",
    "# Here : \n",
    "# 1. we create a dictonary.\n",
    "# 2. Then we created a function, which will retrieve, whatever chat history is there for a specific session id.\n",
    "# 3. Then we are converting/using the RunnableWithMessageHistory, so we can interact with our LM model based on the chat history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke([HumanMessage(content=\"Hi , My name is Divya Gupta and I am a Lead Developer\")],\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Divya Gupta!  \\n\\nIt's great to meet you.  Being a Lead Developer is a challenging and rewarding role. What kind of projects are you currently leading?  \\n\\nI'm curious to hear about the technologies you're using and the problems you're solving.\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Divya Gupta.  \\n\\nI remember from our earlier conversation. ðŸ˜Š  How can I help you today? \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 171, 'total_tokens': 200, 'completion_time': 0.052727273, 'prompt_time': 0.008641406, 'queue_time': 0.16259483400000002, 'total_time': 0.061368679}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--0b593633-8165-4cfb-8832-ec69b7f64de8-0', usage_metadata={'input_tokens': 171, 'output_tokens': 29, 'total_tokens': 200})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations and do not know your name. If you'd like to tell me, I'd be happy to use it! ðŸ˜Š\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Till now you are part of seesion id 1. \n",
    "# Now lets change the session id and see if your chatbot will be able to remember your conversation\n",
    "\n",
    "## change the config-->session id\n",
    "config2={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config2\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi John! It's nice to meet you. ðŸ˜Š \\n\\nWhat can I do for you today?\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feeding information to the session2.\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is John\")],\n",
    "    config=config2\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is John, that's what you told me! ðŸ˜Š  \\n\\nIs there anything else I can help you with?\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Whats my name\")],\n",
    "    config=config2\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
